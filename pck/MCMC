#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jun 26 16:57:24 2025

@author: niyashao
"""

###import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xarray as xr
from pyproj import CRS, Transformer
import verde as vd

from matplotlib.colors import ListedColormap
from sklearn.preprocessing import QuantileTransformer
from sklearn.metrics import pairwise_distances
import gstatsim as gs
import gstools as gstools
from mpl_toolkits.axes_grid1 import make_axes_locatable
import skgstat as skg
from skgstat import models
import random
import time
import math

from PIL import Image, ImageFilter
import cv2

from statsmodels.tsa.stattools import acf


### Geostatistics

def fit_variogram(data, coords, roughness_region_mask, maxlag, n_lags=50, samples=0.6, subsample=100000, data_for_trans = []):

    if len(data_for_trans)==0:
        nst_trans = QuantileTransformer(n_quantiles=500, output_distribution="normal",random_state=0,subsample=subsample).fit(data)
    else:
        nst_trans = QuantileTransformer(n_quantiles=500, output_distribution="normal",random_state=0,subsample=subsample).fit(data_for_trans)
        
    transformed_data = nst_trans.transform(data)
    
    coords = coords[roughness_region_mask==1]
    values = transformed_data[roughness_region_mask==1].flatten()
    
    test1 = skg.Variogram(coords, values, bin_func='even', n_lags=n_lags, 
                    maxlag=maxlag, normalize=False, model='gaussian',samples=samples)
    test2 = skg.Variogram(coords, values, bin_func='even', n_lags=n_lags, 
                       maxlag=maxlag, normalize=False, model='exponential',samples=samples)
    test3 = skg.Variogram(coords, values, bin_func='even', n_lags=n_lags, 
                       maxlag=maxlag, normalize=False, model='spherical',samples=samples)

    tp1 = test1.parameters
    tp2 = test2.parameters
    tp3 = test3.parameters

    print('range, sill, and nugget for gaussian variogram is ', tp1)
    print('for exponential variogram is ', tp2)
    print('for spherical variogram is ', tp3)
    
    # extract experimental variogram values
    xdata1 = test1.bins
    ydata1 = test1.experimental
    xdata2 = test2.bins
    ydata2 = test2.experimental
    xdata3 = test3.bins
    ydata3 = test3.experimental

    # evaluate models
    xi = np.linspace(0, xdata2[-1], n_lags) 
    y_gauss = [models.gaussian(h, tp1[0], tp1[1], tp1[2]) for h in xi]
    y_exp = [models.exponential(h, tp2[0], tp2[1], tp2[2]) for h in xi]
    y_sph = [models.spherical(h, tp3[0], tp3[1], tp3[2]) for h in xi]

    # plot variogram model
    fig = plt.figure(figsize=(6,4))
    plt.plot(xi, y_gauss,'b--', label='Gaussian variogram')
    plt.plot(xi, y_exp,'b-', label='Exponential variogram')
    plt.plot(xi, y_sph,'b*-', label='Spherical variogram')
    plt.plot(xdata1, ydata1,'o', markersize=4, color='green', label='Experimental variogram gaussian')
    plt.plot(xdata2, ydata2,'o', markersize=4, color='orange', label='Experimental variogram exponential')
    plt.plot(xdata3, ydata3,'o', markersize=4, color='pink', label='Experimental variogram spherical')
    plt.title('Variogram for synthetic data')
    plt.xlabel('Lag [m]'); plt.ylabel('Semivariance')  
    plt.legend(loc='lower right')
    
    return nst_trans, transformed_data, [tp1, tp2, tp3], fig


#generate random Gaussian field
def get_random_field(X,Y,step_range,nug_max,range_min,range_max,model_name='Gaussian',_mean=0,_var=1,isotropic=False):
    rng = np.random.default_rng()
    scale  = rng.uniform(low=np.min(step_range), high=np.max(step_range), size=1)[0]/3
    nug = rng.uniform(low=0.0, high=nug_max, size=1)[0]
    if not isotropic:
        range1 = rng.uniform(low=range_min[0], high=range_max[0], size=1)[0]
        range2 = rng.uniform(low=range_min[1], high=range_max[1], size=1)[0]
        angle = rng.uniform(low=0, high=180, size=1)[0]
    else:
        range1 = rng.uniform(low=range_min[0], high=range_max[0], size=1)[0]
        range2 = range1
        angle = 0.0
        
    if model_name == 'Gaussian':
        model = gstools.Gaussian(dim=2, var = _var,
                            len_scale = [range1/np.sqrt(3),range2/np.sqrt(3)],
                            angles = angle*np.pi/180,
                            nugget = nug)
    elif model_name == 'Exponential':
        #a bug here, when exponential, lenscale should be /3, not /sqrt(3)
        model = gstools.Exponential(dim=2, var = _var,
                    len_scale = [range1/np.sqrt(3),range2/np.sqrt(3)],
                    angles = angle*np.pi/180,
                    nugget = nug)
    else:
        print('error model name')
        return

    srf = gstools.SRF(model)
    field = srf.structured([X, Y]).T*scale + _mean

    return field


#logistic related functions
#min_dist, rescale, logistic, get_crf_wegith are written by michael
def min_dist(hard_mat, xx, yy):
    dist = np.zeros(xx.shape)
    xx_hard = np.where(np.isnan(hard_mat), np.nan, xx)
    yy_hard = np.where(np.isnan(hard_mat), np.nan, yy)
    
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1]):
            dist[i,j] = np.nanmin(np.sqrt(np.square(yy[i,j]-yy_hard)+np.square(xx[i,j]-xx_hard)))
    return dist

def rescale(x, maxdist):
    return np.where(x>maxdist,1,(x/maxdist))

def logistic(x, L, x0, k):
    return L/(1+np.exp(-k*(x-x0)))

def get_crf_weight(xx,yy,logistic_param,cond_data_mask,max_dist):
    dist = min_dist(np.where(cond_data_mask==0, np.nan, 1), xx, yy)
    dist_rescale = rescale(dist, max_dist)
    dist_logi = logistic(dist_rescale, logistic_param[0], logistic_param[1], logistic_param[2]) - logistic_param[3]

    weight = dist_logi - np.min(dist_logi)
    return weight, dist, dist_rescale, dist_logi

def get_crf_weight_from_dist(xx,yy,logistic_param,cond_data_mask,max_dist,dist):
    dist_rescale = rescale(dist, max_dist)
    dist_logi = logistic(dist_rescale, logistic_param[0], logistic_param[1], logistic_param[2]) - logistic_param[3]

    weight = dist_logi - np.min(dist_logi)
    return weight, dist, dist_rescale, dist_logi


#sample by field perturbation
def get_edge_mask(min_block_w, max_block_w, min_block_h, max_block_h, logistic_param, maxdist, res, num_step = 5):
    
    width = np.linspace(min_block_w,max_block_w,num_step,dtype=int)
    height = np.linspace(min_block_h,max_block_h,num_step,dtype=int)
    w,h = np.meshgrid(width,height)
    pairs = np.array([(w//2*2).flatten(),(h//2*2).flatten()])

    edge_masks = []

    for i in range(pairs.shape[1]):
        bwidth = pairs[:,i][0]
        bheight = pairs[:,i][1]
        xx,yy=np.meshgrid(range(bwidth),range(bheight))
        xx = xx*res #grid cell resolution, such that maxdist is appropriate
        yy = yy*res
        cond_msk_edge = np.zeros((bheight,bwidth))
        cond_msk_edge[0,:]=1
        cond_msk_edge[bheight-1,:]=1
        cond_msk_edge[:,0]=1
        cond_msk_edge[:,bwidth-1]=1
        dist_edge = min_dist(np.where(cond_msk_edge==0, np.nan, 1), xx, yy)
        dist_rescale_edge = rescale(dist_edge, maxdist)
        dist_logi_edge = logistic(dist_rescale_edge, logistic_param[0], logistic_param[1], logistic_param[2]) - logistic_param[3]
        edge_masks.append(dist_logi_edge)

    return pairs, edge_masks

### MCMC

def loss_map(mc_res, loss_type, inside_high_vel_region = True, high_vel_mask = -1):
    
    if inside_high_vel_region:
        if high_vel_mask.shape != mc_res.shape:
            raise Exception('if inside_high_vel_region is True, then the high_vel_mask need to be defined')
    else:
        high_vel_mask = np.full(mc_res.shape,1)
    
    if loss_type == 'meanabs':
        loss = np.nanmean(np.abs(mc_res[high_vel_mask==1]))
    elif loss_type == 'meansq':
        loss = np.nanmean(np.square(mc_res[high_vel_mask==1]))
    elif loss_type == 'sumabs':
        loss = np.nansum(np.abs(mc_res[high_vel_mask==1]))
    elif loss_type == 'sumsq':
        loss = np.nansum(np.square(mc_res[high_vel_mask==1]))
    else:
        raise Exception("the loss_type parameter is not correct")

    return loss

def loss_diff(bed, conditioning_data, data_mask, loss_type, inside_high_vel_region = True, high_vel_mask = -1, outlier_delta = -1):
    if bed.shape != conditioning_data.shape:
        raise Exception('the bed and the conditioning data have to be the same size')

    if bed.shape != data_mask.shape:
        raise Exception('the bed and the data_mask have to be the same size')
    
    if inside_high_vel_region:
        if high_vel_mask.shape != bed.shape:
            raise Exception('if inside_high_vel_region is True, then the high_vel_mask need to be defined')
    else:
        high_vel_mask = np.full(bed.shape,1)
    
    mask_bed = bed[(data_mask==1)&(high_vel_mask==1)]
    cond_v = conditioning_data[(data_mask==1)&(high_vel_mask==1)]
    
    if loss_type == 'meanabs':
        loss = np.nanmean(np.abs(mask_bed-cond_v))
    elif loss_type == 'meansq':
        loss = np.nanmean(np.square(mask_bed-cond_v))
    elif loss_type == 'sumabs':
        loss = np.nansum(np.abs(mask_bed-cond_v))
    elif loss_type == 'sumsq':
        loss = np.nansum(np.square(mask_bed-cond_v))
    elif loss_type == 'huber':
        if outlier_delta < 0:
            raise Exception('outlier_delta, which is required for huber loss, is not set correctly. It need to be a positive value')
        else:
            res = mask_bed - cond_v
            huber = []
            for r in res:
                if np.abs(r) <= outlier_delta:
                    huber.append(np.square(r)*0.5)
                else:
                    huber.append(outlier_delta*(np.abs(r)-(0.5*outlier_delta)))
        loss = np.nansum(huber)
    else:
        raise Exception("the loss_type parameter is not correct")

    return loss


#should be the parameter of the chain's class
#xx, yy
#bed, surf, velx, vely, dhdt, smb
#cond_bed, data_mask, resolution

#should be parameter of the function
#n_iter, block_size, rfgen_param, random_field_model, isotropic, 

#set_update_type()
# if type = 'rbf_CRF'
# if type = 'logi_CRF'
# if type = 'RF'

#set_update_region(within_region = True/False, region_mask = [])
# if within_region = True:
#   check region_mask

#set_loss_type(res = True/False, diff = True/False, map_func = defaultFunc, diff_func = defaultFunc)
#make it a wrapper of the general MCMC loss, I mean if people want use it for other MCMC they can use pyMC

#passing loss function as a parameter https://www.geeksforgeeks.org/python/passing-function-as-an-argument-in-python/


#
#this function should also be inheritable
#

#sample by CRF created by logi function
#sample by RF: CRF with logi weight = 1 -> RF
#sample in high velocity region
#sample in the entire studying domain
#sample with different loss functions
#with or without difference in data variance or/and mc variance in high velocity region or not

def sample_both_block_logi(bed, surf, velx, vely, dhdt, smb,
                           cond_bed, data_mask, resolution,
                           n_iter, block_size, rfgen_param, mc_loss_type, sigma_mc, 
                           logistic_param = -1, mc_region_mask = -1, sigma_data = -1, maxdist = -1, crf_weight = -1,
                           data_loss_type = 'default', data_loss = False, CRF = True, dataloss_in_high_vel = True,
                           mcloss_in_high_vel = True, onlychange_in_highvel = True,
                           outlier_delta = -1, isotropic=False, model_name='Gaussian'):

    #check input parameter
    if (bed.shape!=surf.shape) or (bed.shape!=velx.shape) or (bed.shape!=vely.shape) or (bed.shape!=dhdt.shape) or (bed.shape!=smb.shape) or (bed.shape!=cond_bed.shape) or (bed.shape!=data_mask.shape):
        raise Exception('the shape of bed, surf, velx, vely, dhdt, smb, radar_bed, data_mask need to be same')
    if (data_loss) and (sigma_data <= 0):
        raise Exception('if include data loss, the sigma_data have to be positive')
    if CRF and ((maxdist <= 0) or (len(logistic_param)!=4 or (crf_weight.shape!=bed.shape))):
        raise Exception('if update with conditional field, the maxdist need to be greater than 0 and the logistic param need be set to 4 numbers [L, x0, k, offset] and crf_weight need to have a same shape as bed')
    if (dataloss_in_high_vel or mcloss_in_high_vel or onlychange_in_highvel) and (mc_region_mask.shape!=bed.shape):
        raise Exception('if sample in_high_vel_region, then mc_region_mask need to be properly defined')

        
    # initialize storage
    loss_mc_cache = np.zeros(n_iter)
    loss_data_cache = np.zeros(n_iter)
    loss_cache = np.zeros(n_iter)
    step_cache = np.zeros(n_iter)
    bed_cache = np.zeros((n_iter, bed.shape[0], bed.shape[1]))
    blocks_cache = np.full((n_iter, 4), np.nan)
    resampled_times = np.zeros(bed.shape)
    

    #if loss is limited by high vel region mask
    if not (dataloss_in_high_vel or mcloss_in_high_vel or onlychange_in_highvel):
        mc_region_mask = np.full(bed.shape,1)

    # initialize loss
    mc_res = calc_mass_conservation(bed, surf, velx, vely, dhdt, smb)

    loss_prev_mc = loss_mc(mc_res, mc_loss_type, inside_high_vel_region = mcloss_in_high_vel, high_vel_mask = mc_region_mask)
    if data_loss:
        loss_prev_data = loss_data(bed, cond_bed, data_mask, data_loss_type, 
                                   inside_high_vel_region = dataloss_in_high_vel, high_vel_mask = mc_region_mask, 
                                   outlier_delta = outlier_delta)
        loss_prev = (loss_prev_mc / (2*sigma_mc**2)) + (loss_prev_data / (2*sigma_data**2))
    else:
        loss_prev = (loss_prev_mc / (2*sigma_mc**2))

    loss_cache[0] = loss_prev
    if data_loss:
        loss_data_cache[0] = loss_prev_data
    loss_mc_cache[0] = loss_prev_mc
    step_cache[0] = False
    bed_cache[0] = bed
    
    # get param
    range_max = rfgen_param[0]
    range_min = rfgen_param[1]
    step_range = rfgen_param[2]
    nug_max = rfgen_param[3]
    res = resolution
    
    rng = np.random.default_rng()
    
    min_block = block_size[0]
    max_block = block_size[1]

    #generate all possible field size and mask
    whpairs, edge_masks = get_edge_mask(min_block[0],max_block[0],min_block[1],max_block[1],logistic_param,maxdist,resolution)

    for i in range(1,n_iter):
        
        #choose block size
        block_size_i = rng.integers(low=0, high=whpairs.shape[1], size=1)[0]
        block_size = whpairs[:,block_size_i]

        if onlychange_in_highvel:
            while True:
                indexx = rng.integers(low=0, high=mc_region_mask.shape[0], size=1)[0]
                indexy = rng.integers(low=0, high=mc_region_mask.shape[1], size=1)[0]
                if mc_region_mask[indexx,indexy] == 1:
                    break
        else:
            indexx = rng.integers(low=0, high=bed.shape[0], size=1)[0]
            indexy = rng.integers(low=0, high=bed.shape[1], size=1)[0]

        #find the index of the block side, make sure the block is within the edge of the map
        bxmin = np.max((0,int(indexx-block_size[1]/2)))
        bxmax = np.min((bed.shape[0],int(indexx+block_size[1]/2)))
        bymin = np.max((0,int(indexy-block_size[0]/2)))
        bymax = np.min((bed.shape[1],int(indexy+block_size[0]/2)))

        #record block
        blocks_cache[i,:]=[indexx,indexy,block_size[0],block_size[1]]

        #generate field
        x_uniq = np.arange(0,block_size[0]*res,res)
        y_uniq = np.arange(0,block_size[1]*res,res)

        #in-case of a weird bug
        while True:
            f = generate_RF(x_uniq,y_uniq,step_range,nug_max,range_min,range_max,model_name=model_name,isotropic=isotropic)
            if (np.sum(np.isnan(f))) != 0:
                print('f have nan')
                continue
            else:
                break

        #find the index of the block side in the coordinate of the block
        mxmin = np.max([f.shape[0]-bxmax,0])
        mxmax = np.min([bed.shape[0]-bxmin,f.shape[0]])
        mymin = np.max([f.shape[1]-bymax,0])
        mymax = np.min([bed.shape[1]-bymin,f.shape[1]])
        
        #perturb
        if CRF:
            perturb = (f*edge_masks[block_size_i])[mxmin:mxmax,mymin:mymax]*crf_weight[bxmin:bxmax,bymin:bymax]
        else:
            perturb = (f*edge_masks[block_size_i])[mxmin:mxmax,mymin:mymax]
        
        bed_next = bed.copy()
        bed_next[bxmin:bxmax,bymin:bymax]=bed_next[bxmin:bxmax,bymin:bymax] + perturb
        
        if onlychange_in_highvel:
            bed_next = np.where(mc_region_mask, bed_next, bed)

        #make sure no bed elevation is greater than surface elevation
        #bed_next = np.where(bed_next>surf, surf, bed_next)

        #calculate new MC error only in the fast region
        mc_res = calc_mass_conservation(bed_next, surf, velx, vely, dhdt, smb)
        
        #loss
        loss_next_mc = loss_mc(mc_res, mc_loss_type, inside_high_vel_region = mcloss_in_high_vel, 
                               high_vel_mask = mc_region_mask)
        if data_loss:
            loss_next_data = loss_data(bed_next, cond_bed, data_mask, data_loss_type, 
                                       inside_high_vel_region = dataloss_in_high_vel, high_vel_mask = mc_region_mask, 
                                       outlier_delta = outlier_delta)
            loss_next = (loss_next_mc / (2*sigma_mc**2)) + (loss_next_data / (2*sigma_data**2))
        else:
            loss_next = (loss_next_mc / (2*sigma_mc**2))
            
        #make sure no bed elevation is greater than surface elevation
        block_thickness = surf[bxmin:bxmax,bymin:bymax] - bed_next[bxmin:bxmax,bymin:bymax]
        block_mc_region_mask = mc_region_mask[bxmin:bxmax,bymin:bymax]
        if np.sum((block_thickness<=0)[block_mc_region_mask==1]) > 0:
            loss_next = np.inf

        if loss_prev > loss_next:
            acceptance_rate = 1
        else:
            acceptance_rate = min(1,np.exp(loss_prev-loss_next))
        
        u = np.random.rand()
        if (u <= acceptance_rate):
            bed = bed_next
            
            loss_prev = loss_next
            loss_prev_mc = loss_next_mc
            loss_cache[i] = loss_next
            loss_mc_cache[i] = loss_next_mc

            if data_loss:
                loss_prev_data = loss_next_data
                loss_data_cache[i] = loss_next_data
            
            step_cache[i] = True
            resampled_times[bxmin:bxmax,bymin:bymax] += mc_region_mask[bxmin:bxmax,bymin:bymax]
            
        else:
            loss_mc_cache[i] = loss_prev_mc
            loss_cache[i] = loss_prev
            if data_loss:
                loss_data_cache[i] = loss_prev_data
            step_cache[i] = False

        bed_cache[i,:,:] = bed
        
        if i%100 == 0:
            print(f'i: {i} mc loss: {loss_mc_cache[i]:.3e} data loss: {loss_data_cache[i]:.3e} loss: {loss_cache[i]:.3e} acceptance rate: {np.sum(step_cache[np.max([0,i-1000]):i])/(np.min([i,1000]))}') #to window acceptance rate

    if data_loss:
        return bed_cache, loss_mc_cache, loss_data_cache, loss_cache, step_cache, resampled_times, blocks_cache
    else:
        return bed_cache, loss_mc_cache, loss_cache, step_cache, resampled_times, blocks_cache
    
    
def sgs_chain(psimdf, X_name, Y_name, Z_name, nst_trans, trend, resolution, variogram,
              data_mask, mc_region_mask,
              surf, velx, vely, dhdt, smb,
              n_iter, block_size, 
              mc_loss_type, sigma_mc, mcloss_in_high_vel,
              searching_radius, num_nearest_neighbors,
              rand_dropout, dropoutrate):
    
    #make sure psimdf only have three columns specified by the names

    rng = np.random.default_rng()
    
    min_block = block_size[0]
    max_block = block_size[1]
    rows = len(np.unique(psimdf[Y_name]))
    cols = len(np.unique(psimdf[X_name]))
    
    xmin = np.min(psimdf[X_name])
    xmax = np.max(psimdf[X_name])
    ymin = np.min(psimdf[Y_name])
    ymax = np.max(psimdf[Y_name])
    
    loss_cache = np.zeros(n_iter)
    loss_mc_cache = np.zeros(n_iter)
    step_cache = np.zeros(n_iter)
    bed_cache = np.zeros((n_iter, rows, cols))
    blocks_cache = np.full((n_iter, 4), np.nan)
    
    psimdf['data_mask'] = data_mask.flatten()
    data_index = psimdf[psimdf['data_mask']==1].index
    psimdf['mc_region_mask'] = mc_region_mask.flatten()
    mask_index = psimdf[psimdf['mc_region_mask']==1].index
    
    psimdf['resampled_times']=0
    
    bed = nst_trans.inverse_transform(np.array(psimdf[Z_name]).reshape(-1,1)).reshape(rows,cols) + trend
    mc_res = calc_mass_conservation(bed, surf, velx, vely, dhdt, smb)
    loss_prev_mc = loss_mc(mc_res, mc_loss_type, inside_high_vel_region = mcloss_in_high_vel, high_vel_mask = mc_region_mask)
    loss_prev = (loss_prev_mc / (2*sigma_mc**2))

    loss_cache[0] = loss_prev
    loss_mc_cache[0] = loss_prev_mc
    step_cache[0] = False
    bed_cache[0] = bed
    
    for i in range(n_iter):

        rsm_center_index = mask_index[int(np.random.rand()*len(mask_index))]
        rsm_x_center = psimdf.loc[rsm_center_index,'X']
        rsm_y_center = psimdf.loc[rsm_center_index,'Y']

        block_size_x = rng.integers(low=min_block[0], high=max_block[0], size=1)[0]
        block_size_x = int(block_size_x/2)*resolution #half of the block size
        block_size_y = rng.integers(low=min_block[1], high=max_block[1], size=1)[0]
        block_size_y = int(block_size_y/2)*resolution

        blocks_cache[i,:]=[rsm_x_center,rsm_y_center,block_size_x,block_size_y]

        #left corner in terms of meters
        rsm_x_min = np.max([int(rsm_x_center - block_size_x),xmin])
        rsm_x_max = np.min([int(rsm_x_center + block_size_x),xmax])
        rsm_y_min = np.max([int(rsm_y_center - block_size_y),ymin])
        rsm_y_max = np.min([int(rsm_y_center + block_size_y),ymax])

        rsm_x_dim = rsm_x_max - rsm_x_min
        rsm_y_dim = rsm_y_max - rsm_y_min

        resampling_box_index = psimdf[(rsm_x_min<=psimdf[X_name])&(psimdf[X_name]<rsm_x_max)&(rsm_y_min<=psimdf[Y_name])&(psimdf[Y_name]<rsm_y_max)].index
        if rand_dropout == True:
            intersect_index = resampling_box_index.intersection(data_index)
            intersect_index = np.random.choice(intersect_index, size=int(intersect_index.shape[0]*(1-dropoutrate)), replace=False, p=None)
            drop_index = resampling_box_index.difference(intersect_index)
        else:
            drop_index = resampling_box_index.difference(data_index)

        new_df = psimdf[~psimdf.index.isin(drop_index)].copy()

        Pred_grid_xy_change = gs.Gridding.prediction_grid(rsm_x_min, rsm_x_max - resolution, rsm_y_min, rsm_y_max - resolution, resolution)
        x = np.reshape(Pred_grid_xy_change[:,0], (len(Pred_grid_xy_change[:,0]), 1))
        y = np.flip(np.reshape(Pred_grid_xy_change[:,1], (len(Pred_grid_xy_change[:,1]), 1)))
        Pred_grid_xy_change = np.concatenate((x,y),axis=1)

        sim2 = gs.Interpolation.okrige_sgs(Pred_grid_xy_change, new_df, X_name, Y_name, Z_name, num_nearest_neighbors, variogram, searching_radius, quiet=True) 

        xy_grid = np.concatenate((Pred_grid_xy_change[:,0].reshape(-1,1),Pred_grid_xy_change[:,1].reshape(-1,1),np.array(sim2).reshape(-1,1)),axis=1)

        psimdf_next = psimdf.copy()
        psimdf_next.loc[resampling_box_index,[X_name,Y_name,Z_name]] = xy_grid
        bed_next = nst_trans.inverse_transform(np.array(psimdf_next[Z_name]).reshape(-1,1)).reshape(rows,cols) + trend
        
        #make sure no bed elevation is greater than surface elevation
        #bed_next = np.where(bed_next>surf, surf, bed_next)
        
        mc_res = calc_mass_conservation(bed_next, surf, velx, vely, dhdt, smb)
            
        loss_next_mc = loss_mc(mc_res, mc_loss_type, inside_high_vel_region = mcloss_in_high_vel, high_vel_mask = mc_region_mask)
        loss_next = (loss_next_mc / (2*sigma_mc**2))
        
        #make sure no bed elevation is greater than surface elevation
        thickness = surf - bed_next
        if np.sum((thickness<=0)[mc_region_mask==1]) > 0:
            loss_next = np.inf
        
        if loss_prev > loss_next:
            acceptance_rate = 1
        else:
            acceptance_rate = min(1,np.exp(loss_prev-loss_next))

        u = np.random.rand()
        if (u <= acceptance_rate):
            bed = bed_next
            psimdf = psimdf_next
            loss_cache[i] = loss_next
            loss_prev = loss_next
            loss_mc_cache[i] = loss_next_mc
            loss_prev_mc = loss_next_mc
            step_cache[i] = True
            loss_prev = loss_next
            psimdf.loc[drop_index, 'resampled_times'] = psimdf.loc[drop_index, 'resampled_times'] + 1
        else:
            loss_cache[i] = loss_prev
            loss_mc_cache[i] = loss_prev_mc
            step_cache[i] = False
        bed_cache[i,:,:] = bed

        if i % 100 == 0:
            print(f'i: {i} mc loss: {loss_mc_cache[i]:.3e} loss: {loss_cache[i]:.3e} acceptance rate: {np.sum(step_cache)/(i+1)}')

    resampled_times = psimdf.resampled_times.values.reshape((rows,cols))
            
    return bed_cache, loss_mc_cache, loss_cache, step_cache, resampled_times, blocks_cache